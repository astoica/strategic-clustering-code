{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10144c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This code implements the set of algorithms described in Section 5 from http://www.columbia.edu/~as5001/strategicclustering.pdf in order to find the next point in the fairness/utility-quality trade-off curve.'''\n",
    "import networkx as nx\n",
    "import csv \n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math \n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7b5b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function cluster_proportions() computes the proportions of different communities in each cluster; \n",
    "# the community affiliation of a node is embedded in a graph attribute called 'color' ('b' for majority, 'r' for minority); for using the real data, embed as such.\n",
    "def cluster_proportions(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    # sizes of clusters                                                                                                    \n",
    "    unique, counts = np.unique(cluster_assignment, return_counts=True)\n",
    "    cluster_sizes = dict(zip(unique, counts))\n",
    "    # cluster_proportion is a dictionary mapping from the clusters to the proportion of the majority nodes in each cluster \n",
    "    cluster_proportion = {}\n",
    "    # cluster_majority is a dictionary mapping from the clusters to the number of the majority nodes in each cluster\n",
    "    cluster_majority = {}\n",
    "    # cluster_mminority is a dictionary mapping from the clusters to the number of the minority nodes in each cluster\n",
    "    cluster_minority = {}\n",
    "    for kk in range(no_clusters):\n",
    "        cluster_proportion[kk] = 0\n",
    "        cluster_majority[kk] = 0\n",
    "        cluster_minority[kk] = 0\n",
    "\n",
    "    for u in range(len(list_nodes_G)):\n",
    "        if G.nodes[list_nodes_G[u]]['color'] == 'b':\n",
    "            cluster_proportion[cluster_assignment[u]] += 1\n",
    "            cluster_majority[cluster_assignment[u]] += 1\n",
    "        else:\n",
    "            cluster_minority[cluster_assignment[u]] += 1\n",
    "\n",
    "    for kk in range(no_clusters):\n",
    "        cluster_proportion[kk] /= cluster_sizes[kk]\n",
    "    return cluster_sizes, cluster_proportion, cluster_majority, cluster_minority\n",
    "\n",
    "# the function graph_communities() finds the ratio of majority and minority nodes in the graph\n",
    "# the community affiliation of a node is embedded in a graph attribute called 'color' ('b' for majority, 'r' for minority); for using the real data, embed as such.\n",
    "def graph_communities(G, list_nodes_G):\n",
    "    maj_no = 0\n",
    "    min_no = 0\n",
    "    for u in range(len(list_nodes_G)):\n",
    "        if G.nodes[list_nodes_G[u]]['color'] == 'b':\n",
    "            maj_no += 1\n",
    "        else:\n",
    "            min_no += 1\n",
    "    return maj_no, min_no\n",
    "\n",
    "\n",
    "# the function compute_fairness_linear() computes a linear fairness metric defined as abs(#R in cluster - #B in cluster times (1-p)/p), where p = |B| / (|R| + |B|) in the general graph G\n",
    "def compute_fairness_linear(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    fairness_clusters = {}\n",
    "    [cluster_sizes, cluster_proportion, cluster_majority, cluster_minority] = cluster_proportions(G,list_nodes_G, no_clusters, cluster_assignment)\n",
    "    [maj_no, min_no] = graph_communities(G, list_nodes_G)\n",
    "    maj_prop = maj_no / (maj_no + min_no)\n",
    "    for kk in range(no_clusters):\n",
    "        fairness_clusters[kk] = abs(cluster_minority[kk]- cluster_majority[kk]* (1 - maj_prop)/maj_prop)\n",
    " \n",
    "    fairness_overall = np.sum(list(fairness_clusters.values()))\n",
    "    return fairness_clusters, fairness_overall\n",
    "\n",
    "\n",
    "\n",
    "# the function compute_fairness_avgprop() computes the average fairness as defined by balance for a clustering as defined in http://proceedings.mlr.press/v97/kleindessner19b.html (avg balance)\n",
    "def compute_fairness_avgprop(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    fairness_clusters = {}\n",
    "    [cluster_sizes, cluster_proportion, cluster_majority, cluster_minority] = cluster_proportions(G,list_nodes_G, no_clusters, cluster_assignment)\n",
    "    [maj_no, min_no] = graph_communities(G, list_nodes_G)\n",
    "    maj_prop = maj_no / (maj_no + min_no)\n",
    "    for kk in range(no_clusters):\n",
    "        fairness_clusters[kk] = abs(cluster_proportion[kk]- maj_prop)\n",
    " \n",
    "    fairness_overall = np.mean(list(fairness_clusters.values()))\n",
    "    return fairness_clusters, fairness_overall\n",
    "\n",
    "# computing the incluster degree of nodes, helper function for compute_fairness_avgprop_mfhg and compute_util_avgprop_closeness\n",
    "def incluster_degree(G, list_nodes_G, cluster_assignment, node):\n",
    "    degu = 0\n",
    "    for nbr in G.neighbors(node):\n",
    "        if cluster_assignment[list_nodes_G.index(node)] == cluster_assignment[list_nodes_G.index(nbr)]:\n",
    "            degu += 1\n",
    "    return degu\n",
    "\n",
    "\n",
    "# the function compute_fairness_avgprop_mfhg() computes the average utility as defined by mfu (https://www.semanticscholar.org/paper/Price-of-Pareto-Optimality-in-Hedonic-Games-Elkind-Fanelli/7764a4ee7d8e0a56c05439a429678d031ce601d4) \n",
    "# this utility function is mfu, from Price of Pareto Optimality in hedonic games by elkind et al, namely, w_i(C) / |C| - 1 (or w_i(C)/|C|) where w_i(C) is the sum of utility of node i in cluster C\n",
    "def compute_util_avgprop_mfhg(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    util_clusters = {}\n",
    "    for i in range(no_clusters):\n",
    "        util_clusters[i] = 0\n",
    "        # indices in cluster i\n",
    "        cl = np.where(cluster_assignment == i)[0]\n",
    "        for j in cl: \n",
    "            deg_u = incluster_degree(G, list_nodes_G, cluster_assignment, list_nodes_G[j])\n",
    "            util_clusters[i] += deg_u/ (len(cl) - 1)\n",
    "    util_overall = np.mean(list(util_clusters.values()))\n",
    "    return util_clusters, util_overall\n",
    "\n",
    "# the function compute_util_avgprop_closeness() computes the average utility as defined by the closeness utility, section 3 in http://www.columbia.edu/~as5001/strategicclustering.pdf\n",
    "def compute_util_avgprop_closeness(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    util_clusters = {}\n",
    "    for i in range(no_clusters):\n",
    "        util_clusters[i] = 0\n",
    "        # indices in cluster i\n",
    "        cl = np.where(cluster_assignment == i)[0]\n",
    "        for j in cl: \n",
    "            lengths_total = nx.single_source_shortest_path_length(G, list_nodes_G[j])\n",
    "            deg_u = incluster_degree(G, list_nodes_G, cluster_assignment, list_nodes_G[j])\n",
    "            lengths_j = 0\n",
    "            for jj in cl:\n",
    "                if j != jj:\n",
    "                    lengths_j += lengths_total[list_nodes_G[jj]]\n",
    "            util_clusters[i] += deg_u/lengths_j\n",
    "    util_overall = np.mean(list(util_clusters.values()))\n",
    "    return util_clusters, util_overall\n",
    "\n",
    "# the function compute_conductance() computes average conductance for a clustering assignment (a metric for 'quality' of clustering in networks)\n",
    "def compute_avg_conductance(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    conductance = 0\n",
    "    conductance_clusters = {}\n",
    "    \n",
    "    for i in range(no_clusters):\n",
    "        ll = np.where(cluster_assignment==i)[0].tolist()\n",
    "        conductance_clusters[i] = nx.conductance(G, ll, list(set(list_nodes_G) - set(ll)))\n",
    "    conductance = np.mean(list(conductance_clusters.values()))\n",
    "    return conductance_clusters, conductance\n",
    "\n",
    "# the function compute_avg_kdistance_cl() computes the average distance to the k-means center obtained from the kmeans algorithm for a specified cluster\n",
    "# note: k-means doesn't optimize for average, this is just for our own experiments; in general, we use compute_avg_kdistance_cl_inertia defined below \n",
    "def compute_avg_kdistance_cl(cluster_assignment, my_cluster, km_distances):\n",
    "    avgdist = km_distances[np.where(cluster_assignment==my_cluster)][:,my_cluster].mean()\n",
    "    return avgdist\n",
    "\n",
    "# the function compute_avg_kdistance() computes the average distance of all clusters to their respective k-means centers obtained from the kmeans algorithm\n",
    "# note: k-means doesn't optimize for average, this is just for our own experiments; in general, we use compute_avg_kdistance_cl_inertia defined below \n",
    "def compute_avg_kdistance(cluster_assignment, no_clusters, km_distances):\n",
    "    dist_clusters = {}\n",
    "                                                                                                     \n",
    "    for kkk in range(no_clusters):\n",
    "        dist_clusters[kkk] = km_distances[np.where(cluster_assignment==kkk)][:,kkk].mean()\n",
    "    avgdist = np.mean(list(dist_clusters.values()))\n",
    "    return dist_clusters, avgdist\n",
    "\n",
    "# the function compute_avg_kdistance_cl() computes the average distance to the k-means center obtained from the kmeans algorithm for a specified cluster\n",
    "# note: this is what k-means optimizes for \n",
    "def compute_avg_kdistance_cl_inertia(cluster_assignment, my_cluster, km_distances):\n",
    "    sumdist = km_distances[np.where(cluster_assignment==my_cluster)][:,my_cluster].sum()\n",
    "    return sumdist\n",
    "\n",
    "# the function compute_avg_kdistance() computes the average distance of all clusters to their respective k-means centers obtained from the kmeans algorithm\n",
    "# note: this is what k-means optimizes for \n",
    "def compute_avg_kdistance_inertia(cluster_assignment, no_clusters, km_distances):\n",
    "    dist_clusters = {}\n",
    "                                                                                                     \n",
    "    for kkk in range(no_clusters):\n",
    "        dist_clusters[kkk] = (km_distances[np.where(cluster_assignment==kkk)][:,kkk]).sum()\n",
    "    sumdist = np.sum(list(dist_clusters.values()))\n",
    "    return dist_clusters, sumdist\n",
    "\n",
    "# # the function compute_avg_ncut() computes the normalized cut size between a subgraph S and a graph G\n",
    "def compute_avg_ncut(G, list_nodes_G, no_clusters, cluster_assignment):\n",
    "    ncut = 0\n",
    "    ncut_clusters = {}\n",
    "                                                                                                    \n",
    "    for i in range(no_clusters):\n",
    "        ll = np.where(cluster_assignment==i)[0].tolist()\n",
    "        ncut_clusters[i] = nx.normalized_cut_size(G, ll, list(set(list_nodes_G) - set(ll)))\n",
    "    ncut = np.mean(list(ncut_clusters.values()))\n",
    "    return ncut_clusters, ncut\n",
    "\n",
    "def compute_centroids(G, list_nodes_G, no_clusters, cluster_assignment,myU):\n",
    "    list_centroids = []\n",
    "    for kk in range(no_clusters):\n",
    "        list_centroids.append(myU[np.where(cluster_assignment==kk),:][0].mean(axis=0))\n",
    "    return list_centroids\n",
    "    \n",
    "# the function doubly_weighted_G() creates a transformed doubly-weighted graph from an original inputted graph, where the weights represent the difference fairness/utility and quality, respectively.\n",
    "def doubly_weighted_G(G,Gnew, list_nodes_G,cluster_assignment,no_clusters, km_distances):\n",
    "    # use closeness utility\n",
    "    #[fairness_cl, fairness_all] = compute_util_avgprop_closeness(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "    # use statistical parity\n",
    "    [fairness_cl, fairness_all] = compute_fairness_linear(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "    [distances_cl, distances] = compute_avg_kdistance_inertia(cluster_assignment, no_clusters, km_distances)\n",
    "\n",
    "    for u in range(len(list_nodes_G)):\n",
    "        for v in range(len(list_nodes_G)):\n",
    "            if cluster_assignment[u] != cluster_assignment[v]:\n",
    "                cluster_assignment_copy = copy.deepcopy(cluster_assignment)\n",
    "                mycl = cluster_assignment[v]\n",
    "                # TODO: double check this, don't exchange them just compute the new clustering fairness/quality since you're changing the cluster assignment \n",
    "                cluster_assignment_copy[u], cluster_assignment_copy[v] = cluster_assignment[v], cluster_assignment[u]\n",
    "\n",
    "                # use closeness utility\n",
    "                #[fairness_cl_new, fairness_all_new] = compute_util_avgprop_closeness(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "                # use statistical parity\n",
    "                [fairness_cl_new, fairness_all_new] = compute_fairness_linear(G, list_nodes_G, no_clusters, cluster_assignment_copy)\n",
    "                distances_cl_new = compute_avg_kdistance_cl_inertia(cluster_assignment_copy, mycl, km_distances)\n",
    "\n",
    "                Gnew.add_edge(list_nodes_G[u],list_nodes_G[v],auv=(fairness_cl_new[mycl]-fairness_cl[mycl]),tuv=distances_cl_new-distances_cl[mycl])\n",
    "                #cluster_assignment[u], cluster_assignment[v] = cluster_assignment[v], cluster_assignment[u]\n",
    "    # adding edges from nodes to clusters\n",
    "    cluster_nodes = list(range(len(G.nodes()),len(G.nodes()) + no_clusters))\n",
    "    Gnew.add_nodes_from(cluster_nodes)\n",
    "    for u in range(len(list_nodes_G)):\n",
    "        for i in range(no_clusters):\n",
    "            if cluster_assignment[u] != i:\n",
    "                ucl = cluster_assignment[u]\n",
    "                cluster_assignment[u] = i\n",
    "                v = i + len(G.nodes())\n",
    "                # use closeness utility\n",
    "                #[fairness_cl_new, fairness_all_new] = compute_util_avgprop_closeness(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "                # use statistical parity\n",
    "                [fairness_cl_new, fairness_all_new] = compute_fairness_linear(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "                distances_cl_new = compute_avg_kdistance_cl_inertia(cluster_assignment, i, km_distances)               \n",
    "                Gnew.add_edge(list_nodes_G[u],v,auv=(fairness_cl_new[i]-fairness_cl[i]),tuv=distances_cl_new-distances_cl[i])\n",
    "                cluster_assignment[u] = ucl\n",
    "    # add node start \n",
    "    nn = len(Gnew.nodes())\n",
    "    Gnew.add_node(nn)\n",
    "\n",
    "    for j in cluster_nodes:\n",
    "        Gnew.add_edge(j,nn,auv=0,tuv=0)\n",
    "        Gnew.add_edge(nn,j,auv=0,tuv=0)\n",
    "\n",
    "    # TODO: REMOVE THIS: not necessary to have an edge from start to every node\n",
    "    # for j in range(len(G.nodes())):\n",
    "        # add an edge from start node to every node with weight equal to the difference if we remove the node from its cluster\n",
    "    #    mycl2 = cluster_assignment[j]\n",
    "    #    cluster_assignment[j] = (mycl2 + 1)%no_clusters\n",
    "        # use closeness utility\n",
    "        #[fairness_cl_new2, fairness_all_new2] = compute_util_avgprop_closeness(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "        # use statistical parity\n",
    "    #    [fairness_cl_new2, fairness_all_new2] = compute_fairness_linear(G, list_nodes_G, no_clusters, cluster_assignment)\n",
    "    #    distances_cl_new2 = compute_avg_kdistance_cl_inertia(cluster_assignment, mycl2, km_distances)\n",
    "    #    Gnew.add_edge(nn,list_nodes_G[j],auv=(fairness_cl_new2[mycl2]-fairness_cl[mycl2]),tuv=distances_cl_new2-distances_cl[mycl2])\n",
    "    #    cluster_assignment[j] = mycl2\n",
    "    #for j in range(len(G.nodes()),nn):\n",
    "    #    Gnew.add_edge(j,nn,auv=0,tuv=0)\n",
    "    \n",
    "    return Gnew\n",
    "\n",
    "# this is the classic Floyd-Warshall algorithm for finding whether there is a negative cycle in t_uv weights\n",
    "def negCyclefloydWarshall(G,list_nodes_G): \n",
    "    V = len(G.nodes())\n",
    "    # dist[][] will be the \n",
    "    # output matrix that will  \n",
    "    # finally have the shortest  \n",
    "    # distances between every \n",
    "    # pair of vertices  \n",
    "    #dist=[[0 for i in range(V+1)]for j in range(V+1)] \n",
    "    dist=[[0 for i in range(V)]for j in range(V)] \n",
    "       \n",
    "    # Initialize the solution \n",
    "    # matrix same as input \n",
    "    # graph matrix. Or we can \n",
    "    # say the initial values  \n",
    "    # of shortest distances \n",
    "    # are based on shortest  \n",
    "    # paths considering no \n",
    "    # intermediate vertex.  \n",
    "    for i in range(V): \n",
    "        for j in range(V): \n",
    "            if [list_nodes_G[i],list_nodes_G[j]] in G.edges():\n",
    "                dist[i][j] = G[i][j]['tuv'] \n",
    "            else:\n",
    "                dist[i][j] = math.inf\n",
    "    ''' Add all vertices one \n",
    "        by one to the set of  \n",
    "        intermediate vertices. \n",
    "    ---> Before start of a iteration, \n",
    "         we have shortest \n",
    "        distances between all pairs \n",
    "        of vertices such  \n",
    "        that the shortest distances \n",
    "        consider only the \n",
    "        vertices in set {0, 1, 2, .. k-1} \n",
    "        as intermediate vertices. \n",
    "    ----> After the end of a iteration, \n",
    "          vertex no. k is  \n",
    "        added to the set of \n",
    "        intermediate vertices and  \n",
    "        the set becomes {0, 1, 2, .. k} '''\n",
    "    for k in range(V): \n",
    "      \n",
    "        # Pick all vertices  \n",
    "        # as source one by one \n",
    "        for i in range(V): \n",
    "                   \n",
    "            # Pick all vertices as \n",
    "            # destination for the \n",
    "            # above picked source \n",
    "            for j in range(V): \n",
    "          \n",
    "                # If vertex k is on \n",
    "                # the shortest path from \n",
    "                # i to j, then update \n",
    "                # the value of dist[i][j] \n",
    "                if ((dist[i][k] + dist[k][j]) < dist[i][j]): \n",
    "                        dist[i][j] = dist[i][k] + dist[k][j] \n",
    "   \n",
    "    # If distance of any \n",
    "    # vertex from itself \n",
    "    # becomes negative, then \n",
    "    # there is a negative \n",
    "    # weight cycle. \n",
    "    for i in range(V): \n",
    "        if (dist[i][i] < 0): \n",
    "            return True\n",
    "   \n",
    "    return False\n",
    "\n",
    "# the function create_M_graph() creates a transformed graph with weights a_uv * M - t_uv\n",
    "def create_M_graph(G,newG, theM):\n",
    "    myG_M = nx.DiGraph()\n",
    "    myG_M.add_nodes_from(G)\n",
    "    auv_var=nx.get_edge_attributes(newG,'auv')\n",
    "    tuv_var=nx.get_edge_attributes(newG,'tuv')\n",
    "    for e in newG.edges():\n",
    "        # note that it is auv + M * tuv, in order to make M positive\n",
    "        myG_M.add_edge(e[0],e[1],weight=auv_var[e]+theM*tuv_var[e])\n",
    "    return myG_M\n",
    "\n",
    "# the function SPFA() implements a faster version of the Floyd-Warshall algorithm for finding negative cycles, without early termination and with cycle termination\n",
    "def SPFA(G):\n",
    "    queue = []\n",
    "    for v in G.nodes():\n",
    "        length[v] = 0\n",
    "        dis[v] = 0\n",
    "        pre[v] = 0\n",
    "        queue.append(v)\n",
    "    while len(queue) > 0:\n",
    "        u = queue.pop(0)\n",
    "        for (u, v) in G.edges():\n",
    "            if dis[u] + G[u][v]['weight'] < dis[v]:\n",
    "                pre[v] = u\n",
    "                length[v] = length[u] + 1\n",
    "                if length[v] == len(G.nodes()):\n",
    "                    return v,pre,\"negative cycle detected\"\n",
    "                dis[v] = dis[u] + G[u][v]['weight']\n",
    "                if v not in queue:\n",
    "                    queue.append(v)\n",
    "    return \"no negative cycle detected\"\n",
    "\n",
    "# the function Trace() traces the negative cycle from the vertex given by SPFA\n",
    "def Trace(pre, v):\n",
    "    mys = []\n",
    "    while v not in mys:\n",
    "        mys.append(v)\n",
    "        v = pre[v]\n",
    "    cycle = [v]\n",
    "    while mys[len(mys)-1] != v:\n",
    "        cycle.append(mys.pop())\n",
    "    cycle.append(v)\n",
    "    return cycle\n",
    "\n",
    "# the function compute_slope() computes the slope of line in Euclidean space\n",
    "def compute_slope(x1, y1, x2, y2):\n",
    "    return (float)(y2-y1)/(x2-x1)\n",
    "\n",
    "# the function SPFA() implements a faster version of the Floyd-Warshall algorithm for finding negative cycles, without early termination and with cycle termination, used only for the t-weights\n",
    "def SPFA2(G):\n",
    "    queue = []\n",
    "    for v in G.nodes():\n",
    "        length[v] = 0\n",
    "        dis[v] = 0\n",
    "        pre[v] = 0\n",
    "        queue.append(v)\n",
    "    while len(queue) > 0:\n",
    "        u = queue.pop(0)\n",
    "        for (u, v) in G.edges():\n",
    "            if dis[u] + G[u][v]['tuv'] < dis[v]:\n",
    "                pre[v] = u\n",
    "                length[v] = length[u] + 1\n",
    "                if length[v] == len(G.nodes()):\n",
    "                    return v,pre,\"negative cycle detected\"\n",
    "                dis[v] = dis[u] + G[u][v]['tuv']\n",
    "                if v not in queue:\n",
    "                    queue.append(v)\n",
    "    return \"no negative cycle detected\"\n",
    "\n",
    "# the function compute_conductance() computes average conductance for a clustering assignment\n",
    "def compute_conductance(G, list_of_nodes_G, cluster_assignment, no_of_clusters):\n",
    "    clconductance = {}\n",
    "    for i in range(no_of_clusters):\n",
    "        icl = [list_of_nodes_G[x] for x in list(np.where(cluster_assignment == i)[0])]\n",
    "        #print(icl)\n",
    "        if len(icl) == 0 or len(icl) == len(list_of_nodes_G):\n",
    "            clconductance[i] = 0\n",
    "        else:\n",
    "            clconductance[i] = nx.conductance(G,icl)\n",
    "    if(len([v for v in clconductance.values() if v > 0]) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - sum(clconductance.values())/len([v for v in clconductance.values() if v > 0])\n",
    "\n",
    "# the function compute_balance is computing average balance of a graph and a clustering as defined by balance for a clustering as defined in http://proceedings.mlr.press/v97/kleindessner19b.html (avg balance)\n",
    "def compute_balance(G,list_of_nodes_G,cluster_assignment,no_of_clusters, sensitive_info):\n",
    "   balance_avg = 0\n",
    "   for cl in range(no_of_clusters):\n",
    "       \n",
    "       clind = np.where(cluster_assignment == cl)[0]\n",
    "       sens0cl = 0\n",
    "       sens1cl = 0\n",
    "       for j in clind:\n",
    "           if sensitive_info[j] == 0:\n",
    "               sens0cl += 1\n",
    "           else:\n",
    "               sens1cl +=1\n",
    "       if sens0cl == 0 or sens1cl == 0:\n",
    "           balance_cl = 0\n",
    "       else:\n",
    "           balance_cl = min(sens0cl/sens1cl,sens1cl/sens0cl)\n",
    "       balance_avg += balance_cl\n",
    "   return balance_avg/no_of_clusters\n",
    "\n",
    "# the function compute_balancemin is computing min balance of a graph and a clustering as defined by balance for a clustering as defined in http://proceedings.mlr.press/v97/kleindessner19b.html (avg balance)\n",
    "def compute_balancemin(G,list_of_nodes_G,cluster_assignment,no_of_clusters, sensitive_info):\n",
    "    balance_min = []\n",
    "    for cl in range(no_of_clusters):\n",
    "        clind = np.where(cluster_assignment == cl)[0]\n",
    "        sens0cl = 0\n",
    "        sens1cl = 0\n",
    "        for j in clind:\n",
    "            if sensitive_info[j] == 0:\n",
    "                sens0cl += 1\n",
    "            else:\n",
    "                sens1cl +=1\n",
    "        if sens0cl == 0 or sens1cl == 0:\n",
    "            balance_cl = 0\n",
    "        else:\n",
    "            balance_cl = min(sens0cl/sens1cl,sens1cl/sens0cl)\n",
    "        balance_min.append(balance_cl)\n",
    "    return min(balance_min)\n",
    "\n",
    "\n",
    "# this function is a Python-version of the Matlab code of the fair spectral clustering algorithm in http://proceedings.mlr.press/v97/kleindessner19b.html, used for comparing clustering with our own alg\n",
    "def Fair_SC_normalized(G, adj,no_clusters,sensitive):\n",
    "    n = np.shape(adj)[1]\n",
    "    sens_unique = np.unique(sensitive)    \n",
    "    h = len(sens_unique)\n",
    "    sensitiveNEW=sensitive.copy()\n",
    "    \n",
    "    temp = 0\n",
    "    \n",
    "    for ell in sens_unique:\n",
    "        sensitiveNEW[np.where(sensitive==ell)[0]] = temp\n",
    "        temp += 1\n",
    "\n",
    "    F=np.zeros([n,h-1])\n",
    "    for ell in range(h-1):\n",
    "        temp = np.where(sensitiveNEW == ell)[0]\n",
    "        F[temp,ell]=1\n",
    "        groupSize = len(temp)\n",
    "        F[:,ell] = F[:,ell]-groupSize/n\n",
    "\n",
    "    L = nx.normalized_laplacian_matrix(G)\n",
    "    L.todense()\n",
    "    D = np.diag(np.sum(np.array(adj.todense()), axis=1))\n",
    "\n",
    "    _,Z = null(F.transpose())\n",
    "    zz = ((Z.transpose()).dot(D)).dot(Z)\n",
    "    Q = scipy.linalg.sqrtm(zz)\n",
    "    Q = Q.real\n",
    "    Qinv = np.linalg.inv(Q)\n",
    "    \n",
    "    Msymm = ((((Qinv.transpose()).dot(Z.transpose())).dot(L.todense())).dot(Z)).dot(Qinv)\n",
    "    Msymm = (Msymm+Msymm.transpose())/2\n",
    "    e,v = np.linalg.eig(Msymm)\n",
    "    \n",
    "    i = [list(e).index(j) for j in sorted(list(e))[1:no_clusters]]\n",
    "    Y = np.array(v[:, i])\n",
    "    Y = Y.real\n",
    "\n",
    "    H = (Z.dot(Qinv)).dot(Y)\n",
    "    \n",
    "    km_fair = KMeans(init='k-means++', n_clusters=no_clusters, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "    km_fair.fit(H)\n",
    "    X_dist_fair = km_fair.transform(H)**2\n",
    "\n",
    "    clusterLabels = km_fair.labels_\n",
    "    return clusterLabels\n",
    "\n",
    "def null(a, rtol=1e-5):\n",
    "    u, s, v = np.linalg.svd(a)\n",
    "    rank = (s > rtol*s[0]).sum()\n",
    "    return rank, v[rank:].T.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e308b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # this code is an example for running a simulated model, generated by the stochastic block model \n",
    "# # main function to execute\n",
    "# if __name__ == \"__main__\":\n",
    "#     no_of_iterations = 30\n",
    "#     #iteration = 0\n",
    "#     fairness_list = []\n",
    "#     cost_list = []\n",
    "#     # this code includes the implementation of a stochastic block model; for reading the real data, please see / modify with the code from section5-nashequilibria-realdatasets.py\n",
    "#     # sizes of each block, the length of sizes defines the number of blocks\n",
    "#     sizes = [10, 10, 10, 10, 10]\n",
    "#     # probability of connections \n",
    "#     probs = [[0.7, 0.07, 0.05, 0.03, 0.05], \n",
    "#             [0.07, 0.6, 0.1, 0.05, 0.07], \n",
    "#             [0.05, 0.1, 0.5, 0.05, 0.1], \n",
    "#             [0.03, 0.05, 0.05, 0.6, 0.05], \n",
    "#             [0.05, 0.07, 0.1, 0.05, 0.6]]\n",
    "\n",
    "#     # create the graph based on the stochastic block model\n",
    "#     G_SBM = nx.stochastic_block_model(sizes, probs)\n",
    "\n",
    "#     nx.info(G_SBM)\n",
    "#     list_nodes=list(G_SBM.nodes())\n",
    "\n",
    "#     # add a label, red or blue, to each node, in a random fashion; 'ratio' is the ratio of the red nodes\n",
    "#     ratio = 0.3\n",
    "#     no_red = int(0.3*sum(sizes))\n",
    "#     no_blue = sum(sizes) - no_red\n",
    "#     attributes_r = ['r']*no_red\n",
    "#     attributes_b = ['b']*no_blue\n",
    "#     attributes = attributes_r + attributes_b\n",
    "#     random.shuffle(attributes)\n",
    "#     attributes_dict = {}\n",
    "#     count = 0\n",
    "#     for u in G_SBM.nodes():\n",
    "#         attributes_dict[u] = attributes[count]\n",
    "#         count += 1\n",
    "#     nx.set_node_attributes(G_SBM, attributes_dict,'color')\n",
    "\n",
    "#     # k is the number of clusters for spectral clustering\n",
    "#     k = len(sizes)\n",
    "#     # finding the spectrum of the graph\n",
    "#     A = nx.adjacency_matrix(G_SBM)\n",
    "#     L = nx.normalized_laplacian_matrix(G_SBM)\n",
    "#     L.todense()\n",
    "#     D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "\n",
    "#     e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "#     i = [list(e).index(j) for j in sorted(list(e))[1:k]]\n",
    "#     print(i)\n",
    "\n",
    " \n",
    "#     U = np.array(v[:, i])\n",
    "#     # performing manual spectral clustering: using the first k values of the eigenspace to do k-means \n",
    "#     km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "#     km.fit(U)\n",
    "#     y = km.labels_\n",
    "#     # distances from the k-centers\n",
    "#     X_dist = km.transform(U)**2\n",
    "\n",
    "#     # keeping a copy of the clustering assignment\n",
    "#     y_copy = copy.deepcopy(y)\n",
    "#     print(y_copy)\n",
    "\n",
    "    \n",
    "#     for iteration in range(no_of_iterations):\n",
    "#         print(\"Iteration number: \",iteration)\n",
    "#         # compute the cost and fairness of this clustering\n",
    "#         # using closeness utility\n",
    "#         [_,avgf] = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y)\n",
    "#         # using statistical parity\n",
    "#         #[_,avgf] = compute_fairness_avgprop(G_SBM, list_nodes,k,y)\n",
    "#         print(\"Average unfairness: \", avgf)  \n",
    "        \n",
    "#         fairness_list.append(avgf)\n",
    "#         [_,avgcost] = compute_avg_kdistance(y,k,X_dist)\n",
    "#         print(\"Average cost: \", avgcost)\n",
    "#         cost_list.append(avgcost)\n",
    "#         if avgf == 0:\n",
    "#             break\n",
    "#         # generating G_new with double weights\n",
    "#         G_new = nx.DiGraph()\n",
    "#         G_new.add_nodes_from(G_SBM)\n",
    "#         G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y,k,X_dist)\n",
    "#         list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "#         length = {}\n",
    "#         dis = {}\n",
    "#         pre = {} \n",
    "#         result = SPFA2(G_new)\n",
    "#         # assert that G_new does not have a negative t-cycle\n",
    "#         if len(result) == 3:\n",
    "#             assert(iteration > 0)\n",
    "\n",
    "#             my_slope = compute_slope(fairness_list[iteration-1], cost_list[iteration-1], fairness_list[iteration], cost_list[iteration])\n",
    "#             myM = -1/my_slope\n",
    "#             epsilon = myM*1e-60\n",
    "#             print(\"M and slope:\", M, myM)\n",
    "#             for e in G_new.edges():\n",
    "#                 G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "#                 G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "\n",
    "#         length = {}\n",
    "#         dis = {}\n",
    "#         pre = {} \n",
    "#         newresult = SPFA2(G_new) \n",
    "\n",
    "#         if len(newresult) == 3:\n",
    "#             [vv,pre,stri] = newresult\n",
    "#             negt = Trace(pre,vv)\n",
    "#             print(negt)\n",
    "#             sumcycle = 0\n",
    "#             for i in range(len(negt) - 1):\n",
    "#                 sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "#             print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "#             y_neg = np.copy(y)\n",
    "#             for i in range(len(negt)):\n",
    "#                 if negt[i] < len(G_SBM.nodes()):\n",
    "#                     if i == len(negt) - 1:\n",
    "#                         break\n",
    "#                     if negt[i+1] < len(G_SBM.nodes()):\n",
    "#                         y_neg[negt[i]] = y[negt[i+1]]\n",
    "#                     if negt[i + 1] >= len(G_SBM.nodes()) and negt[i+1] < len(G_SBM.nodes()) + k:\n",
    "#                         y_neg[negt[i]] = negt[i+1] % len(G_SBM.nodes())\n",
    "#             _,avgdistneg = compute_avg_kdistance(y_neg, k, X_dist)\n",
    "#             # using closeness utility\n",
    "#             _,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "#             # using statistical parity\n",
    "#             #_,avgfairnessneg = compute_fairness_avgprop(G_SBM, list_nodes,k,y_neg)\n",
    "#             print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "#             print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)\n",
    "\n",
    "#             break\n",
    "\n",
    "#         length = {}\n",
    "#         dis = {}\n",
    "#         pre = {} \n",
    "\n",
    "#         print(\"we're finding Mlow\")\n",
    "        \n",
    "#         Mfind = 1\n",
    "#         G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#         mresult = SPFA(G_M)\n",
    "#         if(len(mresult) != 3):\n",
    "#             Mfind = 0\n",
    "#             G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#             assert(len(SPFA(G_M)) == 3)\n",
    "#         else:\n",
    "#             while True:\n",
    "#                 G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#                 if SPFA(G_M) == 'no negative cycle detected':\n",
    "#                     break\n",
    "#                 Mfind *= 2\n",
    "\n",
    "#         #print(\"we found Mlow\")\n",
    "#         # initialize M and the limits for the binary search\n",
    "#         M = Mfind/2\n",
    "#         delta = Mfind/2\n",
    "#         low = Mfind/2\n",
    "#         high = Mfind\n",
    "\n",
    "#         # binary search to find M for which there is a cycle of length 0\n",
    "#         termination_condition = 10e-14\n",
    "#         mids = []\n",
    "#         while np.abs(delta) > termination_condition:\n",
    "#             if high > low: \n",
    "#                 mid = (high + low) / 2\n",
    "#                 mids.append(mid)\n",
    "#             G_M = create_M_graph(G_SBM,G_new, mid)\n",
    "#             if SPFA(G_M) == 'no negative cycle detected':\n",
    "#                 delta = mid - low\n",
    "#                 high = mid\n",
    "#             else:\n",
    "#                 delta = high - mid\n",
    "#                 low = mid\n",
    "#         whereinmidsweare = 0\n",
    "#         for mm in reversed(mids):\n",
    "#             whereinmidsweare += 1\n",
    "#             G_M = create_M_graph(G_SBM,G_new, mm)\n",
    "#             if len(SPFA(G_M)) == 3:\n",
    "#                break\n",
    "#         if whereinmidsweare == len(mids):\n",
    "#             break\n",
    "#         M=mm\n",
    "        \n",
    "#         G_M = create_M_graph(G_SBM,G_new, M)\n",
    "#         [v,pre,stri]=SPFA(G_M)\n",
    "#         myc = Trace(pre,v)\n",
    "#         print(\"best cycle: \",myc)\n",
    "#         ytest = np.copy(y)\n",
    "#         for i in range(len(myc)):\n",
    "#             print(myc[i])\n",
    "#             if myc[i] < len(G_SBM.nodes()):\n",
    "#                 if i == len(myc) - 1:\n",
    "#                     break\n",
    "#                 if myc[i+1] < len(G_SBM.nodes()):\n",
    "#                     #a = y[myc[i]]\n",
    "#                     ytest[myc[i]] = y[myc[i+1]]\n",
    "#                 if myc[i + 1] >= len(G_SBM.nodes()) and myc[i+1] < len(G_SBM.nodes()) + k:\n",
    "#                     print(myc[i+1])\n",
    "#                     ytest[myc[i]] = myc[i+1] % len(G_SBM.nodes())\n",
    "#         y = np.copy(ytest)\n",
    "\n",
    "#     print(\"Fairness: \", fairness_list)\n",
    "#     print(\"Cost: \", cost_list)\n",
    "#     filename = 'SBM_n' + str(len(list_nodes)) + '_r' + str(ratio) + '_k' + str(k) + 'closeness'\n",
    "#     f = open('filename','w')\n",
    "#     writer=csv.writer(f,lineterminator=\"\\n\")\n",
    "#     writer.writerow(fairness_list)\n",
    "#     writer.writerow(cost_list)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fd1ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female:  76\n",
      "male:  51\n"
     ]
    }
   ],
   "source": [
    "### the following section reads in one of the datasets: APS, Facebook, Highschool; uncomment for the data desired to use\n",
    "'''#APS dataset: \n",
    "filename = 'APS-clusteringgames-utilities-conductance.csv'\n",
    "\n",
    "# read in the data as a graph\n",
    "G_og = nx.read_gexf('Downloads/clustering_plotting/APS/sampled_APS_pacs052030.gexf')\n",
    "\n",
    "# work with the largest connected compoenent\n",
    "gg = sorted(nx.connected_components(G_og),key=len,reverse=True)[0]\n",
    "Gc = G_og.subgraph(gg)\n",
    "\n",
    "list_nodes=list(Gc.nodes())\n",
    "\n",
    "\n",
    "# finding the spectrum of the graph\n",
    "A = nx.adjacency_matrix(Gc)\n",
    "L = nx.normalized_laplacian_matrix(Gc)\n",
    "L.todense()\n",
    "D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "e, v = np.linalg.eig(L.todense())\n",
    "'''\n",
    "\n",
    "'''#Facebook dataset: \n",
    "filename = 'Facebook-clusteringgames-utilities-conductance.csv'\n",
    "\n",
    "# read in the data as a graph\n",
    "Gc = nx.read_edgelist('Facebook/facebook_combined.txt')\n",
    "\n",
    "list_nodes=list(Gc.nodes())\n",
    "\n",
    "# finding the spectrum of the graph\n",
    "A = nx.adjacency_matrix(Gc)\n",
    "L = nx.normalized_laplacian_matrix(Gc)\n",
    "L.todense()\n",
    "D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "gender = {}\n",
    "egos = ['0', '107','348','414','686','698','1684','1912','3437','3980']\n",
    "genderfeatfinder = {}\n",
    "\n",
    "# find the sensitive feaures (anonymized gender), and place them in a dictionary\n",
    "for u in egos: \n",
    "    genderfeatfinder[u] = {}\n",
    "    filenamefeat = 'Facebook/' + u + '.featnames'\n",
    "    ffeat = open(filenamefeat)\n",
    "    readerfeat = csv.reader(ffeat)\n",
    "    for rowfeat in readerfeat:\n",
    "        myrowfeat = rowfeat[0].split()\n",
    "        genderfeatfinder[u][myrowfeat[0]] = myrowfeat[1].split(';')[0]\n",
    "    ffeat.close()\n",
    "    gender_ind = [k for k,v in genderfeatfinder[u].items() if v == 'gender']\n",
    "    filenameego= 'Facebook/' + u +'.egofeat'\n",
    "    fego = open(filenameego)\n",
    "    readerego =csv.reader(fego)\n",
    "    for rowego in readerego:\n",
    "        myrowego = rowego[0].split()\n",
    "        gender[u] = myrowego[int(max(gender_ind))]\n",
    "    fego.close()\n",
    "    filename= 'Facebook/' + u +'.feat'\n",
    "    f = open(filename)\n",
    "    reader =csv.reader(f)\n",
    "    for row in reader:\n",
    "        myrow = row[0].split()\n",
    "        user = myrow[0]\n",
    "        gender[user] = myrow[int(max(gender_ind))+1]\n",
    "    f.close()\n",
    "\n",
    "# create a list, sensitive[], that encodes the anonymized gender in the data; it is not used in this section\n",
    "sensitive = []\n",
    "for u in list_nodes:\n",
    "    if (gender[u] == '1'):\n",
    "        sensitive.append(1)\n",
    "    else:\n",
    "        sensitive.append(0)\n",
    "sensitive = np.array(sensitive)\n",
    "sensitive\n",
    "'''\n",
    "\n",
    "#Highschool dataset:\n",
    "filename = 'Highschool-clusteringgames-utilities-conductance.csv'\n",
    "\n",
    "# read in the data as a graph\n",
    "G_og = nx.read_edgelist('Friendship-network_data_2013.csv')\n",
    "\n",
    "# get the largest connected component of the graph\n",
    "gg = sorted(nx.connected_components(G_og),key=len,reverse=True)[0]\n",
    "Gbig = G_og.subgraph(gg)\n",
    "G_SBM = Gbig.copy()\n",
    "\n",
    "# find the sensitive features (unanonymized gender) and place it in a dictionary\n",
    "gender = {}\n",
    "\n",
    "filename_read = 'metadata_2013.txt'\n",
    "f = open(filename_read)\n",
    "reader=csv.reader(f)\n",
    "\n",
    "for row in reader:\n",
    "    myrow = row[0].split('\\t')\n",
    "    gender[myrow[0]] = myrow[2]\n",
    "\n",
    "list_init = list(G_SBM.nodes())\n",
    "for u in list_init:\n",
    "    if gender[u] == 'Unknown':\n",
    "        G_SBM.remove_node(u)\n",
    "        \n",
    "# find the spectrum of the graph\n",
    "list_nodes = list(G_SBM.nodes())\n",
    "\n",
    "m = 0 \n",
    "f = 0\n",
    "d = {} \n",
    "for u in G_SBM.nodes():\n",
    "    if gender[u] == 'M':\n",
    "        d[u] = 'b'\n",
    "        m += 1\n",
    "    elif gender[u] == 'F':\n",
    "        d[u] = 'r'\n",
    "        f += 1\n",
    "print('female: ', f)\n",
    "print('male: ', m)\n",
    "\n",
    "nx.set_node_attributes(G_SBM, d, name=\"color\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55b18611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code puts the sensitive attributes in a matrix; this is useful for comparing with the fair spectral clustering algorithm from http://proceedings.mlr.press/v97/kleindessner19b.html\n",
    "sensitive = []\n",
    "for u in list_nodes:\n",
    "    if gender[u] == 'F':\n",
    "        sensitive.append(1)\n",
    "    else:\n",
    "        sensitive.append(0)\n",
    "sensitive = np.array(sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f29c088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph with 127 nodes and 396 edges'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(G_SBM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd98a275",
   "metadata": {},
   "source": [
    "## The next cell should be ran; loops through iterations while fixing negative cycles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the iterations that the trade-off algorithm runs; note that beyond iteration 0, the starting clustering is no longer optimal in the optimization objective (for k-means, that means inertia)\n",
    "no_of_iterations = 6\n",
    "fairness_list = []\n",
    "cost_list = []\n",
    "\n",
    "# k is the number of clusters for spectral clustering\n",
    "#k = len(sizes) # for simulated model\n",
    "k = 2 # for the real data; needs to be varied\n",
    "\n",
    "\n",
    "# computing the spectrum of the graph\n",
    "A = nx.adjacency_matrix(G_SBM)\n",
    "# computing the normalized Laplacian of the graph\n",
    "L = nx.normalized_laplacian_matrix(G_SBM)\n",
    "L.todense()\n",
    "D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "\n",
    "# computing the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "# only take the first k sorted eigenvalues (can start from 0 or from 1, since first is 0 given that we have one connected component often times)\n",
    "i = [list(e).index(j) for j in sorted(list(e))[1:k+1]]\n",
    "print(i)\n",
    "\n",
    "\n",
    "U = np.array(v[:, i])\n",
    "# performing manual spectral clustering: using the first k values of the eigenspace to do k-means \n",
    "#km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0)\n",
    "\n",
    "km.fit(U)\n",
    "y = km.labels_\n",
    "# distances from the k-centers\n",
    "X_dist = km.transform(U)**2\n",
    "\n",
    "# keeping a copy of the clustering assignment\n",
    "y_copy = copy.deepcopy(y)\n",
    "print(y_copy)\n",
    "\n",
    "\n",
    "for iteration in range(no_of_iterations):\n",
    "    print(\"Iteration number: \",iteration)\n",
    "    # compute the cost and fairness of this clustering\n",
    "    # using closeness utility\n",
    "    #[_,avgf] = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y)\n",
    "    # using statistical parity\n",
    "    [_,avgf] = compute_fairness_linear(G_SBM, list_nodes,k,y)\n",
    "    print(\"Average unfairness: \", avgf)  \n",
    "\n",
    "    fairness_list.append(avgf)\n",
    "    [_,avgcost] = compute_avg_kdistance_inertia(y,k,X_dist)\n",
    "    print(\"Average cost: \", avgcost)\n",
    "    cost_list.append(avgcost)\n",
    "    if avgf == 0:\n",
    "        break\n",
    "    # generating G_new with double weights\n",
    "    G_new = nx.DiGraph()\n",
    "    G_new.add_nodes_from(G_SBM)\n",
    "    G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y,k,X_dist)\n",
    "    list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "    result = SPFA2(G_new)\n",
    "    # assert that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we 'perturb' the weights to get rid of this negative t-cycle\n",
    "    if len(result) == 3:\n",
    "        # if we are not at the initial clustering\n",
    "        if iteration > 0:\n",
    "            #my_slope = compute_slope(fairness_list[iteration-1], cost_list[iteration-1], fairness_list[iteration], cost_list[iteration])\n",
    "            #myM = -1/my_slope\n",
    "            #epsilon = myM*1e-60\n",
    "            #print(\"M and slope:\", M, myM)\n",
    "            alpha = (cost_list[iteration-1] - cost_list[iteration]) / (fairness_list[iteration] - fairness_list[iteration - 1])\n",
    "            alpha += 2e-3\n",
    "            print(\"neg cycle, alpha is \", alpha)\n",
    "            print(\"compute metric with alpha for C0: \", cost_list[iteration-1] + alpha*fairness_list[iteration - 1])\n",
    "            print(\"compute metric with alpha for C1: \", cost_list[iteration] + alpha*fairness_list[iteration])\n",
    "            for e in G_new.edges():\n",
    "                \n",
    "                #G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "                G_new[e[0]][e[1]]['tuv'] += alpha* G_new[e[0]][e[1]]['auv']\n",
    "                G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "        # if we are actually at the initial clustering (SC is not the optimal one); can ignore\n",
    "        #if iteration == 0:\n",
    "            #my_slope = compute_slope(0.2, 0, fairness_list[iteration], cost_list[iteration])\n",
    "            #myM = -1/my_slope\n",
    "            #epsilon = myM*1e-60\n",
    "            #print(\"M and slope:\", M, myM)\n",
    "            #for e in G_new.edges():\n",
    "            #    G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "            #    G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "                \n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "    newresult = SPFA2(G_new) \n",
    "\n",
    "    if len(newresult) == 3:\n",
    "        [vv,pre,stri] = newresult\n",
    "        negt = Trace(pre,vv)\n",
    "        print(negt)\n",
    "        sumcycle = 0\n",
    "        t_weights_negt = []\n",
    "        a_weights_negt = []\n",
    "        for i in range(len(negt) - 1):\n",
    "            sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "            t_weights_negt.append(G_new[negt[i]][negt[i+1]]['tuv'])\n",
    "            a_weights_negt.append(G_new[negt[i]][negt[i+1]]['auv'])\n",
    "        print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "        print(\"neg cycle was not fixed, these are the weights: \", t_weights_negt, a_weights_negt)\n",
    "        y_neg = np.copy(y)\n",
    "        negt2 = copy.deepcopy(negt)\n",
    "        for i in range(len(negt)): \n",
    "            if type(negt[i]) == str:\n",
    "                negt2[i] = list_nodes.index(negt[i])\n",
    "        for i in range(len(negt2)):\n",
    "            if negt2[i] < len(G_SBM.nodes()):\n",
    "                if i == len(negt2) - 1:\n",
    "                    break\n",
    "                if negt2[i+1] < len(G_SBM.nodes()):\n",
    "                    y_neg[negt2[i]] = y[negt2[i+1]]\n",
    "                if negt2[i + 1] >= len(G_SBM.nodes()) and negt2[i+1] < len(G_SBM.nodes()) + k:\n",
    "                    y_neg[negt2[i]] = negt2[i+1] % len(G_SBM.nodes())\n",
    "        _,avgdistneg = compute_avg_kdistance_inertia(y_neg, k, X_dist)\n",
    "        # using closeness utility\n",
    "        #_,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "        # using statistical parity\n",
    "        _,avgfairnessneg = compute_fairness_linear(G_SBM, list_nodes,k,y_neg)\n",
    "        print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "        print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)\n",
    "        print(\"compute metric for following the neg cycle: \", avgdistneg + alpha*avgfairnessneg)\n",
    "\n",
    "        #break\n",
    "\n",
    "    # assert again that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we see what clustering we get if we 'fix' the negative t-cycle\n",
    "    # this is an example of a loop that 'fixes' negative cycles until there are no more (until we get to the optimal cycle in the objective set)\n",
    "    if iteration > 0:\n",
    "        while True:    \n",
    "            G_new = nx.DiGraph()\n",
    "            G_new.add_nodes_from(G_SBM)\n",
    "            G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y_neg,k,X_dist)\n",
    "\n",
    "            for e in G_new.edges():\n",
    "                \n",
    "                #G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "                G_new[e[0]][e[1]]['tuv'] += alpha* G_new[e[0]][e[1]]['auv']\n",
    "                G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "\n",
    "            list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "            length = {}\n",
    "            dis = {}\n",
    "            pre = {} \n",
    "            result = SPFA2(G_new)\n",
    "\n",
    "            if len(result) > 3:\n",
    "                break\n",
    "\n",
    "            [vv,pre,stri] = result\n",
    "            negt = Trace(pre,vv)\n",
    "            print(negt)\n",
    "\n",
    "            sumcycle = 0\n",
    "            for i in range(len(negt) - 1):\n",
    "                sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "            print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "            y_neg2 = np.copy(y_neg)\n",
    "            negt2 = copy.deepcopy(negt)\n",
    "            for i in range(len(negt)): \n",
    "                if type(negt[i]) == str:\n",
    "                    negt2[i] = list_nodes.index(negt[i])\n",
    "            for i in range(len(negt2)):\n",
    "                if negt2[i] < len(G_SBM.nodes()):\n",
    "                    if i == len(negt2) - 1:\n",
    "                        break\n",
    "                    if negt2[i+1] < len(G_SBM.nodes()):\n",
    "                        y_neg2[negt2[i]] = y_neg[negt2[i+1]]\n",
    "                    if negt2[i + 1] >= len(G_SBM.nodes()) and negt2[i+1] < len(G_SBM.nodes()) + k:\n",
    "                        y_neg2[negt2[i]] = negt2[i+1] % len(G_SBM.nodes())\n",
    "\n",
    "            y_neg = copy.deepcopy(y_neg2)\n",
    "            _,avgdistneg = compute_avg_kdistance_inertia(y_neg, k, X_dist)\n",
    "            # using closeness utility\n",
    "            #_,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "            # using statistical parity\n",
    "            _,avgfairnessneg = compute_fairness_linear(G_SBM, list_nodes,k,y_neg)\n",
    "            print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "            print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)\n",
    "            print(\"compute metric for following the neg cycle: \", avgdistneg + alpha*avgfairnessneg)\n",
    "\n",
    "    if iteration > 0:    \n",
    "        y = np.copy(y_neg)\n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "\n",
    "    # we get to this point if we managed to fix the negative cycle (might have to run the previuos loop multiple times) or didn't have one to begin with\n",
    "     \n",
    "    print(\"we're finding Mlow\")\n",
    "\n",
    "    Mfind = 1\n",
    "    G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "    mresult = SPFA(G_M)\n",
    "    if(len(mresult) != 3):\n",
    "        Mfind = 0\n",
    "        G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "        assert(len(SPFA(G_M)) == 3)\n",
    "    else:\n",
    "        while True:\n",
    "            G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "            if SPFA(G_M) == 'no negative cycle detected':\n",
    "                break\n",
    "            Mfind *= 2\n",
    "\n",
    "    # at this point, we perform a binary search \n",
    "    # initialize M and the limits for the binary search\n",
    "    M = Mfind/2\n",
    "    delta = Mfind/2\n",
    "    low = Mfind/2\n",
    "    high = Mfind\n",
    "\n",
    "    # binary search to find M for which there is a cycle of length 0\n",
    "    termination_condition = 10e-12\n",
    "    mids = []\n",
    "    while np.abs(delta) > termination_condition:\n",
    "        if high > low: \n",
    "            mid = (high + low) / 2\n",
    "            mids.append(mid)\n",
    "        G_M = create_M_graph(G_SBM,G_new, mid)\n",
    "        if SPFA(G_M) == 'no negative cycle detected':\n",
    "            delta = mid - low\n",
    "            high = mid\n",
    "        else:\n",
    "            delta = high - mid\n",
    "            low = mid\n",
    "        print(delta)\n",
    "    whereinmidsweare = 0\n",
    "    for mm in reversed(mids):\n",
    "        whereinmidsweare += 1\n",
    "        G_M = create_M_graph(G_SBM,G_new, mm)\n",
    "        if len(SPFA(G_M)) == 3:\n",
    "           break\n",
    "    if whereinmidsweare == len(mids):\n",
    "        break\n",
    "    M=mm\n",
    "\n",
    "    G_M = create_M_graph(G_SBM,G_new, M)\n",
    "    [v,pre,stri]=SPFA(G_M)\n",
    "    # this is the best cycle that, if followed, will get us the next best clustering with the minimal ratio of cost loss to fairness/utility increase\n",
    "    myc = Trace(pre,v)\n",
    "    print(\"best cycle: \",myc)\n",
    "    ytest = np.copy(y)\n",
    "    \n",
    "    myc2 = copy.deepcopy(myc)\n",
    "    # we 'follow' the best cycle in order to find that next best clustering, and will compute the cost and fairness/utility as we start the loop again in the next iteration\n",
    "    for i in range(len(myc)): \n",
    "        if type(myc[i]) == str:\n",
    "            myc2[i] = list_nodes.index(myc[i])\n",
    "    for i in range(len(myc2)):\n",
    "        print(myc2[i])\n",
    "        if myc2[i] < len(G_SBM.nodes()):\n",
    "            if i == len(myc2) - 1:\n",
    "                break\n",
    "            if myc2[i+1] < len(G_SBM.nodes()):\n",
    "                #a = y[myc2[i]]\n",
    "                ytest[myc2[i]] = y[myc2[i+1]]\n",
    "            if myc2[i + 1] >= len(G_SBM.nodes()) and myc2[i+1] < len(G_SBM.nodes()) + k:\n",
    "                print(myc2[i+1])\n",
    "                ytest[myc2[i]] = myc2[i+1] % len(G_SBM.nodes())\n",
    "    y = np.copy(ytest)\n",
    "\n",
    "print(\"Fairness: \", fairness_list)\n",
    "print(\"Cost: \", cost_list)\n",
    "#filename = 'SBM_n' + str(len(list_nodes)) + '_r' + str(ratio) + '_k' + str(k) + 'closeness-test.csv'\n",
    "#filename = 'Highschool_k' + str(k) + 'statparity_test.csv'\n",
    "#f = open(filename,'w')\n",
    "#writer=csv.writer(f,lineterminator=\"\\n\")\n",
    "#writer.writerow(fairness_list)\n",
    "#writer.writerow(cost_list)\n",
    "#f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a930e2f0",
   "metadata": {},
   "source": [
    "## Ignore the next cell, this is for when you don't fix the negative cycles that arise in further iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "441e16ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1]\n",
      "Iteration number:  0\n",
      "Average unfairness:  32.470588235294116\n",
      "Average cost:  1.1903907713067448\n",
      "we're finding Mlow\n",
      "256.0\n",
      "128.0\n",
      "64.0\n",
      "32.0\n",
      "16.0\n",
      "8.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "0.00048828125\n",
      "0.000244140625\n",
      "0.0001220703125\n",
      "6.103515625e-05\n",
      "3.0517578125e-05\n",
      "1.52587890625e-05\n",
      "7.62939453125e-06\n",
      "3.814697265625e-06\n",
      "1.9073486328125e-06\n",
      "9.5367431640625e-07\n",
      "4.76837158203125e-07\n",
      "2.384185791015625e-07\n",
      "1.1920928955078125e-07\n",
      "5.960464477539063e-08\n",
      "2.9802322387695312e-08\n",
      "1.4901161193847656e-08\n",
      "7.450580596923828e-09\n",
      "3.725290298461914e-09\n",
      "1.862645149230957e-09\n",
      "9.313225746154785e-10\n",
      "4.656612873077393e-10\n",
      "2.3283064365386963e-10\n",
      "1.1641532182693481e-10\n",
      "5.820766091346741e-11\n",
      "2.9103830456733704e-11\n",
      "1.4551915228366852e-11\n",
      "7.275957614183426e-12\n",
      "best cycle:  ['92', '222', '92']\n",
      "67\n",
      "106\n",
      "67\n",
      "Iteration number:  1\n",
      "Average unfairness:  27.490196078431374\n",
      "Average cost:  1.19544540455392\n",
      "neg cycle, alpha is  0.0030149066756139297\n",
      "['1401', '845', '1401']\n",
      "negative t-cycle was not fixed:  -0.0015639473\n",
      "Cost when correcting a negative cycle:  1.208896874789178\n",
      "Fairness when correcting a negative cycle:  22.509803921568626\n",
      "Fairness:  [32.470588235294116, 27.490196078431374]\n",
      "Cost:  [1.1903907713067448, 1.19544540455392]\n"
     ]
    }
   ],
   "source": [
    "# # these are the iterations that the trade-off algorithm runs; note that beyond iteration 0, the starting clustering is no longer optimal in the optimization objective (for k-means, that means inertia)\n",
    "# no_of_iterations = 6\n",
    "# fairness_list = []\n",
    "# cost_list = []\n",
    "\n",
    "# # k is the number of clusters for spectral clustering\n",
    "# #k = len(sizes) # for simulated model\n",
    "# k = 2 # for the real data; needs to be varied\n",
    "\n",
    "\n",
    "# # computing the spectrum of the graph\n",
    "# A = nx.adjacency_matrix(G_SBM)\n",
    "# # computing the normalized Laplacian of the graph\n",
    "# L = nx.normalized_laplacian_matrix(G_SBM)\n",
    "# L.todense()\n",
    "# D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "\n",
    "# # computing the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "# e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "# # only take the first k sorted eigenvalues (can start from 0 or from 1, since first is 0 given that we have one connected component often times)\n",
    "# i = [list(e).index(j) for j in sorted(list(e))[1:k+1]]\n",
    "# print(i)\n",
    "\n",
    "\n",
    "# U = np.array(v[:, i])\n",
    "# # performing manual spectral clustering: using the first k values of the eigenspace to do k-means \n",
    "# #km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "# km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0)\n",
    "\n",
    "# km.fit(U)\n",
    "# y = km.labels_\n",
    "# # distances from the k-centers\n",
    "# X_dist = km.transform(U)**2\n",
    "\n",
    "# # keeping a copy of the clustering assignment\n",
    "# y_copy = copy.deepcopy(y)\n",
    "# print(y_copy)\n",
    "\n",
    "\n",
    "# for iteration in range(no_of_iterations):\n",
    "#     print(\"Iteration number: \",iteration)\n",
    "#     # compute the cost and fairness of this clustering\n",
    "#     # using closeness utility\n",
    "#     #[_,avgf] = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y)\n",
    "#     # using statistical parity\n",
    "#     [_,avgf] = compute_fairness_linear(G_SBM, list_nodes,k,y)\n",
    "#     print(\"Average unfairness: \", avgf)  \n",
    "\n",
    "#     fairness_list.append(avgf)\n",
    "#     [_,avgcost] = compute_avg_kdistance_inertia(y,k,X_dist)\n",
    "#     print(\"Average cost: \", avgcost)\n",
    "#     cost_list.append(avgcost)\n",
    "#     if avgf == 0:\n",
    "#         break\n",
    "#     # generating G_new with double weights\n",
    "#     G_new = nx.DiGraph()\n",
    "#     G_new.add_nodes_from(G_SBM)\n",
    "#     G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y,k,X_dist)\n",
    "#     list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "#     length = {}\n",
    "#     dis = {}\n",
    "#     pre = {} \n",
    "#     result = SPFA2(G_new)\n",
    "#     # assert that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we 'perturb' the weights to get rid of this negative t-cycle\n",
    "#     if len(result) == 3:\n",
    "#         # if we are not at the initial clustering\n",
    "#         if iteration > 0:\n",
    "#             #my_slope = compute_slope(fairness_list[iteration-1], cost_list[iteration-1], fairness_list[iteration], cost_list[iteration])\n",
    "#             #myM = -1/my_slope\n",
    "#             #epsilon = myM*1e-60\n",
    "#             #print(\"M and slope:\", M, myM)\n",
    "#             alpha = (cost_list[iteration-1] - cost_list[iteration]) / (fairness_list[iteration] - fairness_list[iteration - 1])\n",
    "#             alpha += 2e-3\n",
    "#             print(\"neg cycle, alpha is \", alpha)\n",
    "#             for e in G_new.edges():\n",
    "                \n",
    "#                 #G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "#                 G_new[e[0]][e[1]]['tuv'] += alpha* G_new[e[0]][e[1]]['auv']\n",
    "#                 G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "#         # if we are actually at the initial clustering (SC is not the optimal one); can ignore\n",
    "#         #if iteration == 0:\n",
    "#             #my_slope = compute_slope(0.2, 0, fairness_list[iteration], cost_list[iteration])\n",
    "#             #myM = -1/my_slope\n",
    "#             #epsilon = myM*1e-60\n",
    "#             #print(\"M and slope:\", M, myM)\n",
    "#             #for e in G_new.edges():\n",
    "#             #    G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "#             #    G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "                \n",
    "#     length = {}\n",
    "#     dis = {}\n",
    "#     pre = {} \n",
    "#     newresult = SPFA2(G_new) \n",
    "\n",
    "#     # assert again that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we see what clustering we get if we 'fix' the negative t-cycle\n",
    "#     if len(newresult) == 3:\n",
    "#         [vv,pre,stri] = newresult\n",
    "#         negt = Trace(pre,vv)\n",
    "#         print(negt)\n",
    "#         sumcycle = 0\n",
    "#         for i in range(len(negt) - 1):\n",
    "#             sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "#         print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "#         y_neg = np.copy(y)\n",
    "#         negt2 = copy.deepcopy(negt)\n",
    "#         for i in range(len(negt)): \n",
    "#             if type(negt[i]) == str:\n",
    "#                 negt2[i] = list_nodes.index(negt[i])\n",
    "#         for i in range(len(negt2)):\n",
    "#             if negt2[i] < len(G_SBM.nodes()):\n",
    "#                 if i == len(negt2) - 1:\n",
    "#                     break\n",
    "#                 if negt2[i+1] < len(G_SBM.nodes()):\n",
    "#                     y_neg[negt2[i]] = y[negt2[i+1]]\n",
    "#                 if negt2[i + 1] >= len(G_SBM.nodes()) and negt2[i+1] < len(G_SBM.nodes()) + k:\n",
    "#                     y_neg[negt2[i]] = negt2[i+1] % len(G_SBM.nodes())\n",
    "#         _,avgdistneg = compute_avg_kdistance_inertia(y_neg, k, X_dist)\n",
    "#         # using closeness utility\n",
    "#         #_,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "#         # using statistical parity\n",
    "#         _,avgfairnessneg = compute_fairness_linear(G_SBM, list_nodes,k,y_neg)\n",
    "#         print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "#         print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)\n",
    "\n",
    "#         break\n",
    "\n",
    "#     length = {}\n",
    "#     dis = {}\n",
    "#     pre = {} \n",
    "\n",
    "#     # we get to this point if we managed to fix the negative cycle (might have to run the previuos loop multiple times) or didn't have one to begin with\n",
    "     \n",
    "#     print(\"we're finding Mlow\")\n",
    "\n",
    "#     Mfind = 1\n",
    "#     G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#     mresult = SPFA(G_M)\n",
    "#     if(len(mresult) != 3):\n",
    "#         Mfind = 0\n",
    "#         G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#         assert(len(SPFA(G_M)) == 3)\n",
    "#     else:\n",
    "#         while True:\n",
    "#             G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#             if SPFA(G_M) == 'no negative cycle detected':\n",
    "#                 break\n",
    "#             Mfind *= 2\n",
    "\n",
    "#     # at this point, we perform a binary search \n",
    "#     # initialize M and the limits for the binary search\n",
    "#     M = Mfind/2\n",
    "#     delta = Mfind/2\n",
    "#     low = Mfind/2\n",
    "#     high = Mfind\n",
    "\n",
    "#     # binary search to find M for which there is a cycle of length 0\n",
    "#     termination_condition = 10e-12\n",
    "#     mids = []\n",
    "#     while np.abs(delta) > termination_condition:\n",
    "#         if high > low: \n",
    "#             mid = (high + low) / 2\n",
    "#             mids.append(mid)\n",
    "#         G_M = create_M_graph(G_SBM,G_new, mid)\n",
    "#         if SPFA(G_M) == 'no negative cycle detected':\n",
    "#             delta = mid - low\n",
    "#             high = mid\n",
    "#         else:\n",
    "#             delta = high - mid\n",
    "#             low = mid\n",
    "#         print(delta)\n",
    "#     whereinmidsweare = 0\n",
    "#     for mm in reversed(mids):\n",
    "#         whereinmidsweare += 1\n",
    "#         G_M = create_M_graph(G_SBM,G_new, mm)\n",
    "#         if len(SPFA(G_M)) == 3:\n",
    "#            break\n",
    "#     if whereinmidsweare == len(mids):\n",
    "#         break\n",
    "#     M=mm\n",
    "\n",
    "#     G_M = create_M_graph(G_SBM,G_new, M)\n",
    "#     [v,pre,stri]=SPFA(G_M)\n",
    "#     # this is the best cycle that, if followed, will get us the next best clustering with the minimal ratio of cost loss to fairness/utility increase\n",
    "#     myc = Trace(pre,v)\n",
    "#     print(\"best cycle: \",myc)\n",
    "#     ytest = np.copy(y)\n",
    "    \n",
    "#     myc2 = copy.deepcopy(myc)\n",
    "#     # we 'follow' the best cycle in order to find that next best clustering, and will compute the cost and fairness/utility as we start the loop again in the next iteration\n",
    "#     for i in range(len(myc)): \n",
    "#         if type(myc[i]) == str:\n",
    "#             myc2[i] = list_nodes.index(myc[i])\n",
    "#     for i in range(len(myc2)):\n",
    "#         print(myc2[i])\n",
    "#         if myc2[i] < len(G_SBM.nodes()):\n",
    "#             if i == len(myc2) - 1:\n",
    "#                 break\n",
    "#             if myc2[i+1] < len(G_SBM.nodes()):\n",
    "#                 #a = y[myc2[i]]\n",
    "#                 ytest[myc2[i]] = y[myc2[i+1]]\n",
    "#             if myc2[i + 1] >= len(G_SBM.nodes()) and myc2[i+1] < len(G_SBM.nodes()) + k:\n",
    "#                 print(myc2[i+1])\n",
    "#                 ytest[myc2[i]] = myc2[i+1] % len(G_SBM.nodes())\n",
    "#     y = np.copy(ytest)\n",
    "\n",
    "# print(\"Fairness: \", fairness_list)\n",
    "# print(\"Cost: \", cost_list)\n",
    "# #filename = 'SBM_n' + str(len(list_nodes)) + '_r' + str(ratio) + '_k' + str(k) + 'closeness-test.csv'\n",
    "# #filename = 'Highschool_k' + str(k) + 'statparity_test.csv'\n",
    "# #f = open(filename,'w')\n",
    "# #writer=csv.writer(f,lineterminator=\"\\n\")\n",
    "# #writer.writerow(fairness_list)\n",
    "# #writer.writerow(cost_list)\n",
    "# #f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67907826",
   "metadata": {},
   "source": [
    "### Testing out the code cell by cell -- ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34a96caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# # these are the iterations that the trade-off algorithm runs; note that beyond iteration 0, the starting clustering is no longer optimal in the optimization objective (for k-means, that means inertia)\n",
    "# no_of_iterations = 6\n",
    "# fairness_list = []\n",
    "# cost_list = []\n",
    "\n",
    "# # k is the number of clusters for spectral clustering\n",
    "# #k = len(sizes) # for simulated model\n",
    "# k = 2 # for the real data; needs to be varied\n",
    "\n",
    "\n",
    "# # computing the spectrum of the graph\n",
    "# A = nx.adjacency_matrix(G_SBM)\n",
    "# # computing the normalized Laplacian of the graph\n",
    "# L = nx.normalized_laplacian_matrix(G_SBM)\n",
    "# L.todense()\n",
    "# D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "\n",
    "# # computing the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "# e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "# # only take the first k sorted eigenvalues (can start from 0 or from 1, since first is 0 given that we have one connected component often times)\n",
    "# i = [list(e).index(j) for j in sorted(list(e))[1:k+1]]\n",
    "# print(i)\n",
    "\n",
    "\n",
    "# U = np.array(v[:, i])\n",
    "# # performing manual spectral clustering: using the first k values of the eigenspace to do k-means \n",
    "# #km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "# km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0)\n",
    "\n",
    "# km.fit(U)\n",
    "# y = km.labels_\n",
    "# # distances from the k-centers\n",
    "# X_dist = km.transform(U)**2\n",
    "\n",
    "# # keeping a copy of the clustering assignment\n",
    "# y_copy = copy.deepcopy(y)\n",
    "# print(y_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcfef3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average unfairness at C0:  32.470588235294116\n",
      "Average cost at C0:  1.1903907713067448\n"
     ]
    }
   ],
   "source": [
    "# [_,avgf] = compute_fairness_linear(G_SBM, list_nodes,k,y)\n",
    "# print(\"Average unfairness at C0: \", avgf)  \n",
    "\n",
    "# fairness_list.append(avgf)\n",
    "# [_,avgcost] = compute_avg_kdistance_inertia(y,k,X_dist)\n",
    "# print(\"Average cost at C0: \", avgcost)\n",
    "# cost_list.append(avgcost)\n",
    "# if avgf == 0:\n",
    "#     print(\"fairness is 0\")\n",
    "# # generating G_new with double weights\n",
    "# G_new = nx.DiGraph()\n",
    "# G_new.add_nodes_from(G_SBM)\n",
    "# G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y,k,X_dist)\n",
    "# list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "# length = {}\n",
    "# dis = {}\n",
    "# pre = {} \n",
    "# result = SPFA2(G_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c9b1f491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no negative cycle detected'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8aa71fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're finding Mlow\n",
      "256.0\n",
      "128.0\n",
      "64.0\n",
      "32.0\n",
      "16.0\n",
      "8.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "0.00048828125\n",
      "0.000244140625\n",
      "0.0001220703125\n",
      "6.103515625e-05\n",
      "3.0517578125e-05\n",
      "1.52587890625e-05\n",
      "7.62939453125e-06\n",
      "3.814697265625e-06\n",
      "1.9073486328125e-06\n",
      "9.5367431640625e-07\n",
      "4.76837158203125e-07\n",
      "2.384185791015625e-07\n",
      "1.1920928955078125e-07\n",
      "5.960464477539063e-08\n",
      "2.9802322387695312e-08\n",
      "1.4901161193847656e-08\n",
      "7.450580596923828e-09\n",
      "3.725290298461914e-09\n",
      "1.862645149230957e-09\n",
      "9.313225746154785e-10\n",
      "4.656612873077393e-10\n",
      "2.3283064365386963e-10\n",
      "1.1641532182693481e-10\n",
      "5.820766091346741e-11\n",
      "2.9103830456733704e-11\n",
      "1.4551915228366852e-11\n",
      "7.275957614183426e-12\n",
      "best cycle:  ['92', '222', '92']\n",
      "67\n",
      "106\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# length = {}\n",
    "# dis = {}\n",
    "# pre = {} \n",
    "\n",
    "# # we get to this point if we managed to fix the negative cycle (might have to run the previuos loop multiple times) or didn't have one to begin with\n",
    "    \n",
    "# print(\"we're finding Mlow\")\n",
    "\n",
    "# Mfind = 1\n",
    "# G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "# mresult = SPFA(G_M)\n",
    "# if(len(mresult) != 3):\n",
    "#     Mfind = 0\n",
    "#     G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#     assert(len(SPFA(G_M)) == 3)\n",
    "# else:\n",
    "#     while True:\n",
    "#         G_M = create_M_graph(G_SBM,G_new, Mfind)\n",
    "#         if SPFA(G_M) == 'no negative cycle detected':\n",
    "#             break\n",
    "#         Mfind *= 2\n",
    "\n",
    "# # at this point, we perform a binary search \n",
    "# # initialize M and the limits for the binary search\n",
    "# M = Mfind/2\n",
    "# delta = Mfind/2\n",
    "# low = Mfind/2\n",
    "# high = Mfind\n",
    "\n",
    "# # binary search to find M for which there is a cycle of length 0\n",
    "# termination_condition = 10e-12\n",
    "# mids = []\n",
    "# while np.abs(delta) > termination_condition:\n",
    "#     if high > low: \n",
    "#         mid = (high + low) / 2\n",
    "#         mids.append(mid)\n",
    "#     G_M = create_M_graph(G_SBM,G_new, mid)\n",
    "#     if SPFA(G_M) == 'no negative cycle detected':\n",
    "#         delta = mid - low\n",
    "#         high = mid\n",
    "#     else:\n",
    "#         delta = high - mid\n",
    "#         low = mid\n",
    "#     print(delta)\n",
    "# whereinmidsweare = 0\n",
    "# for mm in reversed(mids):\n",
    "#     whereinmidsweare += 1\n",
    "#     G_M = create_M_graph(G_SBM,G_new, mm)\n",
    "#     if len(SPFA(G_M)) == 3:\n",
    "#         break\n",
    "#     if whereinmidsweare == len(mids):\n",
    "#         break\n",
    "# M=mm\n",
    "\n",
    "# G_M = create_M_graph(G_SBM,G_new, M)\n",
    "# [v,pre,stri]=SPFA(G_M)\n",
    "# # this is the best cycle that, if followed, will get us the next best clustering with the minimal ratio of cost loss to fairness/utility increase\n",
    "# myc = Trace(pre,v)\n",
    "# print(\"best cycle: \",myc)\n",
    "# ytest = np.copy(y)\n",
    "\n",
    "# myc2 = copy.deepcopy(myc)\n",
    "# # we 'follow' the best cycle in order to find that next best clustering, and will compute the cost and fairness/utility as we start the loop again in the next iteration\n",
    "# for i in range(len(myc)): \n",
    "#     if type(myc[i]) == str:\n",
    "#         myc2[i] = list_nodes.index(myc[i])\n",
    "# for i in range(len(myc2)):\n",
    "#     print(myc2[i])\n",
    "#     if myc2[i] < len(G_SBM.nodes()):\n",
    "#         if i == len(myc2) - 1:\n",
    "#             break\n",
    "#         if myc2[i+1] < len(G_SBM.nodes()):\n",
    "#             #a = y[myc2[i]]\n",
    "#             ytest[myc2[i]] = y[myc2[i+1]]\n",
    "#         if myc2[i + 1] >= len(G_SBM.nodes()) and myc2[i+1] < len(G_SBM.nodes()) + k:\n",
    "#             print(myc2[i+1])\n",
    "#             ytest[myc2[i]] = myc2[i+1] % len(G_SBM.nodes())\n",
    "# y = np.copy(ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4beed424",
   "metadata": {},
   "source": [
    "### Still testing; Now y = C1, compute the new fairness, cost, and check for neg cycles in the doubly weighted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba52f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average unfairness C1:  27.490196078431374\n",
      "Average cost C1:  1.19544540455392\n"
     ]
    }
   ],
   "source": [
    "# [_,avgf] = compute_fairness_linear(G_SBM, list_nodes,k,y)\n",
    "# print(\"Average unfairness C1: \", avgf)  \n",
    "\n",
    "# fairness_list.append(avgf)\n",
    "# [_,avgcost] = compute_avg_kdistance_inertia(y,k,X_dist)\n",
    "# print(\"Average cost C1: \", avgcost)\n",
    "# cost_list.append(avgcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "741b5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generating G_new with double weights\n",
    "# G_new = nx.DiGraph()\n",
    "# G_new.add_nodes_from(G_SBM)\n",
    "# G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y,k,X_dist)\n",
    "# list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "# length = {}\n",
    "# dis = {}\n",
    "# pre = {} \n",
    "# result = SPFA2(G_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "595bcc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127,\n",
       " {'1': '222',\n",
       "  '55': '222',\n",
       "  '205': '222',\n",
       "  '272': '222',\n",
       "  '494': '222',\n",
       "  '779': '222',\n",
       "  '894': '222',\n",
       "  '3': '222',\n",
       "  '28': '222',\n",
       "  '147': '222',\n",
       "  '407': '222',\n",
       "  '674': '222',\n",
       "  '884': '222',\n",
       "  '27': '222',\n",
       "  '63': '92',\n",
       "  '173': '92',\n",
       "  '202': '222',\n",
       "  '327': '222',\n",
       "  '353': '222',\n",
       "  '429': '222',\n",
       "  '441': '222',\n",
       "  '492': '222',\n",
       "  '545': '222',\n",
       "  '32': '92',\n",
       "  '440': '92',\n",
       "  '624': '92',\n",
       "  '797': '92',\n",
       "  '920': '92',\n",
       "  '151': '92',\n",
       "  '277': '92',\n",
       "  '502': '92',\n",
       "  '866': '92',\n",
       "  '45': '222',\n",
       "  '48': '222',\n",
       "  '79': '222',\n",
       "  '335': '222',\n",
       "  '496': '222',\n",
       "  '601': '222',\n",
       "  '765': '222',\n",
       "  '46': '222',\n",
       "  '117': '222',\n",
       "  '196': '222',\n",
       "  '257': '222',\n",
       "  '268': '222',\n",
       "  '170': '222',\n",
       "  '252': '222',\n",
       "  '883': '222',\n",
       "  '61': '92',\n",
       "  '125': '92',\n",
       "  '70': '222',\n",
       "  '101': '222',\n",
       "  '132': '222',\n",
       "  '240': '222',\n",
       "  '425': '222',\n",
       "  '447': '222',\n",
       "  '72': '222',\n",
       "  '857': '222',\n",
       "  '80': '222',\n",
       "  '120': '222',\n",
       "  '285': '222',\n",
       "  '468': '92',\n",
       "  '85': '222',\n",
       "  '190': '222',\n",
       "  '213': '222',\n",
       "  '214': '222',\n",
       "  '603': '222',\n",
       "  '605': '92',\n",
       "  '92': '222',\n",
       "  '845': '222',\n",
       "  '119': '222',\n",
       "  '122': '222',\n",
       "  '343': '222',\n",
       "  '364': '222',\n",
       "  '265': '222',\n",
       "  '465': '222',\n",
       "  '587': '222',\n",
       "  '488': '222',\n",
       "  '255': '92',\n",
       "  '248': '92',\n",
       "  '325': '92',\n",
       "  '491': '92',\n",
       "  '622': '92',\n",
       "  '960': '92',\n",
       "  '134': '222',\n",
       "  '388': '222',\n",
       "  '184': '222',\n",
       "  '38': '92',\n",
       "  '201': '92',\n",
       "  '452': '92',\n",
       "  '634': '92',\n",
       "  '642': '92',\n",
       "  '691': '92',\n",
       "  '694': '92',\n",
       "  '753': '92',\n",
       "  '869': '92',\n",
       "  '156': '92',\n",
       "  '159': '92',\n",
       "  '165': '222',\n",
       "  '498': '222',\n",
       "  '1332': '92',\n",
       "  '200': '222',\n",
       "  '480': '222',\n",
       "  '245': '92',\n",
       "  '211': '222',\n",
       "  '242': '222',\n",
       "  '219': '92',\n",
       "  '222': '92',\n",
       "  '867': '222',\n",
       "  '232': '222',\n",
       "  '798': '92',\n",
       "  '959': '92',\n",
       "  '564': '92',\n",
       "  '275': '92',\n",
       "  '312': '92',\n",
       "  '612': '92',\n",
       "  '769': '92',\n",
       "  '486': '222',\n",
       "  '531': '222',\n",
       "  '771': '222',\n",
       "  '520': '92',\n",
       "  '576': '92',\n",
       "  '577': '92',\n",
       "  '1401': '92',\n",
       "  '1228': '92',\n",
       "  '1519': '92',\n",
       "  '1594': '92',\n",
       "  '1828': '92',\n",
       "  127: 129,\n",
       "  128: '222',\n",
       "  129: 128},\n",
       " 'negative cycle detected')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9fc7c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab749425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1903907713067448, 1.19544540455392]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d31597d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32.470588235294116, 27.490196078431374]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fairness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "127bce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg cycle, alpha is  0.0030149066756139297\n",
      "compute metric with alpha for C0:  1.0924949780750455\n",
      "compute metric with alpha for C1:  1.1125650288831215\n"
     ]
    }
   ],
   "source": [
    "# alpha = (cost_list[iteration-1] - cost_list[iteration]) / (fairness_list[iteration] - fairness_list[iteration - 1])\n",
    "# alpha += 2e-3\n",
    "# print(\"neg cycle, alpha is \", alpha)\n",
    "# print(\"compute metric with alpha for C0: \", cost_list[iteration-1] + alpha*fairness_list[iteration - 1])\n",
    "# print(\"compute metric with alpha for C1: \", cost_list[iteration] + alpha*fairness_list[iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c10ac9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['92', '222', '92']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# myc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2d51f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009960784313725317\n"
     ]
    }
   ],
   "source": [
    "# # myc is the best cycle that we followed \n",
    "# # compute m(myc)\n",
    "# m_myc = 0\n",
    "# for i in range(len(myc) - 1):\n",
    "#      m_myc += G_new[myc[i]][myc[i+1]]['tuv'] + alpha*G_new[myc[i]][myc[i+1]]['auv']\n",
    "# print(m_myc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6ec1ff6",
   "metadata": {},
   "source": [
    "### Example of a loop that 'fixes' negative cycles until there are no more (until we get to the optimal cycle in the objective set); ignore for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87974aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['845', '92', '845']\n",
      "negative t-cycle was not fixed:  -0.006338780461233318\n",
      "Cost when correcting a negative cycle:  1.1911696742660702\n",
      "Fairness when correcting a negative cycle:  30.470588235294116\n",
      "[129, '222', 128, 129]\n",
      "negative t-cycle was not fixed:  -0.0007789029593255892\n",
      "Cost when correcting a negative cycle:  1.1903907713067448\n",
      "Fairness when correcting a negative cycle:  32.470588235294116\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qc/k6jgnl3532x82rw058f_4nj40002pv/T/ipykernel_9298/819209532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mvv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstri\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mnegt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# # this is an example of a loop that 'fixes' negative cycles until there are no more (until we get to the optimal cycle in the objective set)\n",
    "# while True:    \n",
    "#     G_new = nx.DiGraph()\n",
    "#     G_new.add_nodes_from(G_SBM)\n",
    "#     G_new = doubly_weighted_G(G_SBM,G_new,list_nodes,y_neg,k,X_dist)\n",
    "#     list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "#     length = {}\n",
    "#     dis = {}\n",
    "#     pre = {} \n",
    "#     result = SPFA2(G_new)\n",
    "\n",
    "#     if len(result) > 3:\n",
    "#         break\n",
    "\n",
    "#     [vv,pre,stri] = result\n",
    "#     negt = Trace(pre,vv)\n",
    "#     print(negt)\n",
    "\n",
    "#     sumcycle = 0\n",
    "#     for i in range(len(negt) - 1):\n",
    "#         sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "#     print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "#     y_neg2 = np.copy(y_neg)\n",
    "#     negt2 = copy.deepcopy(negt)\n",
    "#     for i in range(len(negt)): \n",
    "#         if type(negt[i]) == str:\n",
    "#             negt2[i] = list_nodes.index(negt[i])\n",
    "#     for i in range(len(negt2)):\n",
    "#         if negt2[i] < len(G_SBM.nodes()):\n",
    "#             if i == len(negt2) - 1:\n",
    "#                 break\n",
    "#             if negt2[i+1] < len(G_SBM.nodes()):\n",
    "#                 y_neg2[negt2[i]] = y_neg[negt2[i+1]]\n",
    "#             if negt2[i + 1] >= len(G_SBM.nodes()) and negt2[i+1] < len(G_SBM.nodes()) + k:\n",
    "#                 y_neg2[negt2[i]] = negt2[i+1] % len(G_SBM.nodes())\n",
    "\n",
    "#     y_neg = copy.deepcopy(y_neg2)\n",
    "#     _,avgdistneg = compute_avg_kdistance_inertia(y_neg, k, X_dist)\n",
    "#     # using closeness utility\n",
    "#     #_,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "#     # using statistical parity\n",
    "#     _,avgfairnessneg = compute_fairness_linear(G_SBM, list_nodes,k,y_neg)\n",
    "#     print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "#     print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "427b11d7",
   "metadata": {},
   "source": [
    "### Re-color the clusters found by spectral clustering, one all red and one all blue, to see how the algorithm changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8d2db75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# these are the iterations that the trade-off algorithm runs; note that beyond iteration 0, the starting clustering is no longer optimal in the optimization objective (for k-means, that means inertia)\n",
    "no_of_iterations = 6\n",
    "fairness_list = []\n",
    "cost_list = []\n",
    "\n",
    "# k is the number of clusters for spectral clustering\n",
    "#k = len(sizes) # for simulated model\n",
    "k = 2 # for the real data; needs to be varied\n",
    "\n",
    "\n",
    "# computing the spectrum of the graph\n",
    "A = nx.adjacency_matrix(G_SBM)\n",
    "# computing the normalized Laplacian of the graph\n",
    "L = nx.normalized_laplacian_matrix(G_SBM)\n",
    "L.todense()\n",
    "D = np.diag(np.sum(np.array(A.todense()), axis=1))\n",
    "\n",
    "# computing the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "e, v = np.linalg.eig(L.todense())\n",
    "\n",
    "# only take the first k sorted eigenvalues (can start from 0 or from 1, since first is 0 given that we have one connected component often times)\n",
    "i = [list(e).index(j) for j in sorted(list(e))[1:k+1]]\n",
    "print(i)\n",
    "\n",
    "\n",
    "U = np.array(v[:, i])\n",
    "# performing manual spectral clustering: using the first k values of the eigenspace to do k-means \n",
    "#km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0, random_state=3425)\n",
    "km = KMeans(init='k-means++', n_clusters=k, max_iter=200, n_init=200, verbose=0)\n",
    "\n",
    "km.fit(U)\n",
    "y = km.labels_\n",
    "# distances from the k-centers\n",
    "X_dist = km.transform(U)**2\n",
    "\n",
    "# keeping a copy of the clustering assignment\n",
    "y_copy = copy.deepcopy(y)\n",
    "print(y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f10b4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_SBM_recolor = copy.deepcopy(G_SBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "138d0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in range(len(list_nodes)):\n",
    "    if y[u] == 0:\n",
    "        G_SBM_recolor.nodes[list_nodes[u]]['color'] = 'b'\n",
    "    else:\n",
    "        G_SBM_recolor.nodes[list_nodes[u]]['color'] = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49179c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  0\n",
      "Average unfairness:  56.0\n",
      "Average cost:  0.2561903917149986\n",
      "we're finding Mlow\n",
      "1024.0\n",
      "512.0\n",
      "256.0\n",
      "128.0\n",
      "64.0\n",
      "32.0\n",
      "16.0\n",
      "8.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "0.5\n",
      "0.25\n",
      "0.125\n",
      "0.0625\n",
      "0.03125\n",
      "0.015625\n",
      "0.0078125\n",
      "0.00390625\n",
      "0.001953125\n",
      "0.0009765625\n",
      "0.00048828125\n",
      "0.000244140625\n",
      "0.0001220703125\n",
      "6.103515625e-05\n",
      "3.0517578125e-05\n",
      "1.52587890625e-05\n",
      "7.62939453125e-06\n",
      "3.814697265625e-06\n",
      "1.9073486328125e-06\n",
      "9.5367431640625e-07\n",
      "4.76837158203125e-07\n",
      "2.384185791015625e-07\n",
      "1.1920928955078125e-07\n",
      "5.960464477539063e-08\n",
      "2.9802322387695312e-08\n",
      "1.4901161193847656e-08\n",
      "7.450580596923828e-09\n",
      "3.725290298461914e-09\n",
      "1.862645149230957e-09\n",
      "9.313225746154785e-10\n",
      "4.656612873077393e-10\n",
      "2.3283064365386963e-10\n",
      "1.1641532182693481e-10\n",
      "5.820766091346741e-11\n",
      "2.9103830456733704e-11\n",
      "1.4551915228366852e-11\n",
      "7.275957614183426e-12\n",
      "best cycle:  [129, '601', 128, 129]\n",
      "129\n",
      "37\n",
      "128\n",
      "128\n",
      "129\n",
      "Iteration number:  1\n",
      "Average unfairness:  55.2112676056338\n",
      "Average cost:  0.25682553263364405\n",
      "neg cycle, alpha is  0.002805267950425506\n",
      "['531', '232', '531']\n",
      "negative t-cycle was not fixed:  -0.0055493168\n",
      "Cost when correcting a negative cycle:  0.2613119631759144\n",
      "Fairness when correcting a negative cycle:  53.4225352112676\n",
      "Fairness:  [56.0, 55.2112676056338]\n",
      "Cost:  [0.2561903917149986, 0.25682553263364405]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(no_of_iterations):\n",
    "    print(\"Iteration number: \",iteration)\n",
    "    # compute the cost and fairness of this clustering\n",
    "    # using closeness utility\n",
    "    #[_,avgf] = compute_util_avgprop_closeness(G_SBM_recolor, list_nodes,k,y)\n",
    "    # using statistical parity\n",
    "    [_,avgf] = compute_fairness_linear(G_SBM_recolor, list_nodes,k,y)\n",
    "    print(\"Average unfairness: \", avgf)  \n",
    "\n",
    "    fairness_list.append(avgf)\n",
    "    [_,avgcost] = compute_avg_kdistance_inertia(y,k,X_dist)\n",
    "    print(\"Average cost: \", avgcost)\n",
    "    cost_list.append(avgcost)\n",
    "    if avgf == 0:\n",
    "        break\n",
    "    # generating G_new with double weights\n",
    "    G_new = nx.DiGraph()\n",
    "    G_new.add_nodes_from(G_SBM_recolor)\n",
    "    G_new = doubly_weighted_G(G_SBM_recolor,G_new,list_nodes,y,k,X_dist)\n",
    "    list_nodes_Gnew = list(G_new.nodes())\n",
    "\n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "    result = SPFA2(G_new)\n",
    "    # assert that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we 'perturb' the weights to get rid of this negative t-cycle\n",
    "    if len(result) == 3:\n",
    "        # if we are not at the initial clustering\n",
    "        if iteration > 0:\n",
    "            #my_slope = compute_slope(fairness_list[iteration-1], cost_list[iteration-1], fairness_list[iteration], cost_list[iteration])\n",
    "            #myM = -1/my_slope\n",
    "            #epsilon = myM*1e-60\n",
    "            #print(\"M and slope:\", M, myM)\n",
    "            alpha = (cost_list[iteration-1] - cost_list[iteration]) / (fairness_list[iteration] - fairness_list[iteration - 1])\n",
    "            alpha += 2e-3\n",
    "            print(\"neg cycle, alpha is \", alpha)\n",
    "            for e in G_new.edges():\n",
    "                \n",
    "                #G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "                G_new[e[0]][e[1]]['tuv'] += alpha* G_new[e[0]][e[1]]['auv']\n",
    "                G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "        # if we are actually at the initial clustering (SC is not the optimal one); can ignore\n",
    "        #if iteration == 0:\n",
    "            #my_slope = compute_slope(0.2, 0, fairness_list[iteration], cost_list[iteration])\n",
    "            #myM = -1/my_slope\n",
    "            #epsilon = myM*1e-60\n",
    "            #print(\"M and slope:\", M, myM)\n",
    "            #for e in G_new.edges():\n",
    "            #    G_new[e[0]][e[1]]['tuv'] += G_new[e[0]][e[1]]['auv']/(myM+epsilon)\n",
    "            #    G_new[e[0]][e[1]]['tuv'] = round(G_new[e[0]][e[1]]['tuv'],10)\n",
    "                \n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "    newresult = SPFA2(G_new) \n",
    "\n",
    "    # assert again that G_new does not have a negative t-cycle; if it does, then len(result) == 3 and then we see what clustering we get if we 'fix' the negative t-cycle\n",
    "    if len(newresult) == 3:\n",
    "        [vv,pre,stri] = newresult\n",
    "        negt = Trace(pre,vv)\n",
    "        print(negt)\n",
    "        sumcycle = 0\n",
    "        for i in range(len(negt) - 1):\n",
    "            sumcycle += G_new[negt[i]][negt[i+1]]['tuv']\n",
    "        print(\"negative t-cycle was not fixed: \", sumcycle)\n",
    "        y_neg = np.copy(y)\n",
    "        negt2 = copy.deepcopy(negt)\n",
    "        for i in range(len(negt)): \n",
    "            if type(negt[i]) == str:\n",
    "                negt2[i] = list_nodes.index(negt[i])\n",
    "        for i in range(len(negt2)):\n",
    "            if negt2[i] < len(G_SBM_recolor.nodes()):\n",
    "                if i == len(negt2) - 1:\n",
    "                    break\n",
    "                if negt2[i+1] < len(G_SBM_recolor.nodes()):\n",
    "                    y_neg[negt2[i]] = y[negt2[i+1]]\n",
    "                if negt2[i + 1] >= len(G_SBM_recolor.nodes()) and negt2[i+1] < len(G_SBM_recolor.nodes()) + k:\n",
    "                    y_neg[negt2[i]] = negt2[i+1] % len(G_SBM_recolor.nodes())\n",
    "        _,avgdistneg = compute_avg_kdistance_inertia(y_neg, k, X_dist)\n",
    "        # using closeness utility\n",
    "        #_,avgfairnessneg = compute_util_avgprop_closeness(G_SBM, list_nodes,k,y_neg)\n",
    "        # using statistical parity\n",
    "        _,avgfairnessneg = compute_fairness_linear(G_SBM_recolor, list_nodes,k,y_neg)\n",
    "        print(\"Cost when correcting a negative cycle: \", avgdistneg)\n",
    "        print(\"Fairness when correcting a negative cycle: \", avgfairnessneg)\n",
    "\n",
    "        break\n",
    "\n",
    "    length = {}\n",
    "    dis = {}\n",
    "    pre = {} \n",
    "\n",
    "    # we get to this point if we managed to fix the negative cycle (might have to run the previuos loop multiple times) or didn't have one to begin with\n",
    "     \n",
    "    print(\"we're finding Mlow\")\n",
    "\n",
    "    Mfind = 1\n",
    "    G_M = create_M_graph(G_SBM_recolor,G_new, Mfind)\n",
    "    mresult = SPFA(G_M)\n",
    "    if(len(mresult) != 3):\n",
    "        Mfind = 0\n",
    "        G_M = create_M_graph(G_SBM_recolor,G_new, Mfind)\n",
    "        assert(len(SPFA(G_M)) == 3)\n",
    "    else:\n",
    "        while True:\n",
    "            G_M = create_M_graph(G_SBM_recolor,G_new, Mfind)\n",
    "            if SPFA(G_M) == 'no negative cycle detected':\n",
    "                break\n",
    "            Mfind *= 2\n",
    "\n",
    "    # at this point, we perform a binary search \n",
    "    # initialize M and the limits for the binary search\n",
    "    M = Mfind/2\n",
    "    delta = Mfind/2\n",
    "    low = Mfind/2\n",
    "    high = Mfind\n",
    "\n",
    "    # binary search to find M for which there is a cycle of length 0\n",
    "    termination_condition = 10e-12\n",
    "    mids = []\n",
    "    while np.abs(delta) > termination_condition:\n",
    "        if high > low: \n",
    "            mid = (high + low) / 2\n",
    "            mids.append(mid)\n",
    "        G_M = create_M_graph(G_SBM_recolor,G_new, mid)\n",
    "        if SPFA(G_M) == 'no negative cycle detected':\n",
    "            delta = mid - low\n",
    "            high = mid\n",
    "        else:\n",
    "            delta = high - mid\n",
    "            low = mid\n",
    "        print(delta)\n",
    "    whereinmidsweare = 0\n",
    "    for mm in reversed(mids):\n",
    "        whereinmidsweare += 1\n",
    "        G_M = create_M_graph(G_SBM_recolor,G_new, mm)\n",
    "        if len(SPFA(G_M)) == 3:\n",
    "           break\n",
    "    if whereinmidsweare == len(mids):\n",
    "        break\n",
    "    M=mm\n",
    "\n",
    "    G_M = create_M_graph(G_SBM_recolor,G_new, M)\n",
    "    [v,pre,stri]=SPFA(G_M)\n",
    "    # this is the best cycle that, if followed, will get us the next best clustering with the minimal ratio of cost loss to fairness/utility increase\n",
    "    myc = Trace(pre,v)\n",
    "    print(\"best cycle: \",myc)\n",
    "    ytest = np.copy(y)\n",
    "    \n",
    "    myc2 = copy.deepcopy(myc)\n",
    "    # we 'follow' the best cycle in order to find that next best clustering, and will compute the cost and fairness/utility as we start the loop again in the next iteration\n",
    "    for i in range(len(myc)): \n",
    "        if type(myc[i]) == str:\n",
    "            myc2[i] = list_nodes.index(myc[i])\n",
    "    for i in range(len(myc2)):\n",
    "        print(myc2[i])\n",
    "        if myc2[i] < len(G_SBM_recolor.nodes()):\n",
    "            if i == len(myc2) - 1:\n",
    "                break\n",
    "            if myc2[i+1] < len(G_SBM_recolor.nodes()):\n",
    "                #a = y[myc2[i]]\n",
    "                ytest[myc2[i]] = y[myc2[i+1]]\n",
    "            if myc2[i + 1] >= len(G_SBM_recolor.nodes()) and myc2[i+1] < len(G_SBM_recolor.nodes()) + k:\n",
    "                print(myc2[i+1])\n",
    "                ytest[myc2[i]] = myc2[i+1] % len(G_SBM_recolor.nodes())\n",
    "    y = np.copy(ytest)\n",
    "\n",
    "print(\"Fairness: \", fairness_list)\n",
    "print(\"Cost: \", cost_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
